{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xqrWupU2ESJ"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 1: Sequence Models\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1640375368123,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "-mNwk59Y2EzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18283,
     "status": "ok",
     "timestamp": 1640375386778,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "laJ3WDtY2F54",
    "outputId": "5a0f605b-9945-4051-f2b6-ce215089486e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1640375387831,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "bykEfCrs2Jmy",
    "outputId": "6a542cc1-f9d8-4104-ada3-2ac0931ca055"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8IotZe02ESN"
   },
   "source": [
    "In this part we will learn about working with text sequences using recurrent neural networks.\n",
    "We'll go from a raw text file all the way to a fully trained GRU-RNN model and generate works of art!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:45.017980Z",
     "iopub.status.busy": "2021-12-28T07:46:45.016640Z",
     "iopub.status.idle": "2021-12-28T07:46:48.254043Z",
     "shell.execute_reply": "2021-12-28T07:46:48.254043Z"
    },
    "executionInfo": {
     "elapsed": 5964,
     "status": "ok",
     "timestamp": 1640375393794,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "fXHpszCM2ESO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:48.258031Z",
     "iopub.status.busy": "2021-12-28T07:46:48.258031Z",
     "iopub.status.idle": "2021-12-28T07:46:48.391676Z",
     "shell.execute_reply": "2021-12-28T07:46:48.391676Z"
    },
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1640375394418,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "bGWpsaxZ2ESQ",
    "outputId": "3884afef-b6e2-4f82-afa1-e2f135b1667c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9r8rp2W2ESR"
   },
   "source": [
    "## Text generation with a char-level RNN\n",
    "<a id=part1_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHVVX4Dc2ESR"
   },
   "source": [
    "### Obtaining the corpus\n",
    "<a id=part1_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqCwpZkY2ESS"
   },
   "source": [
    "Let's begin by downloading a corpus containing all the works of William Shakespeare.\n",
    "Since he was very prolific, this corpus is fairly large and will provide us with enough data for\n",
    "obtaining impressive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:48.397658Z",
     "iopub.status.busy": "2021-12-28T07:46:48.396661Z",
     "iopub.status.idle": "2021-12-28T07:46:48.524320Z",
     "shell.execute_reply": "2021-12-28T07:46:48.523322Z"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1640375394899,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "y47xJjiZ2EST",
    "outputId": "1c243d13-0e68-4f68-d491-a60822d63083",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus file C:\\Users\\x3yusk\\.pytorch-datasets\\shakespeare.txt exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "CORPUS_URL = 'https://github.com/cedricdeboom/character-level-rnn-datasets/raw/master/datasets/shakespeare.txt'\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "\n",
    "def download_corpus(out_path=DATA_DIR, url=CORPUS_URL, force=False):\n",
    "    pathlib.Path(out_path).mkdir(exist_ok=True)\n",
    "    out_filename = os.path.join(out_path, os.path.basename(url))\n",
    "    \n",
    "    if os.path.isfile(out_filename) and not force:\n",
    "        print(f'Corpus file {out_filename} exists, skipping download.')\n",
    "    else:\n",
    "        print(f'Downloading {url}...')\n",
    "        with urllib.request.urlopen(url) as response, open(out_filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "        print(f'Saved to {out_filename}.')\n",
    "    return out_filename\n",
    "    \n",
    "corpus_path = download_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odMgUm_S2ESW"
   },
   "source": [
    "Load the text into memory and print a snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:48.528309Z",
     "iopub.status.busy": "2021-12-28T07:46:48.528309Z",
     "iopub.status.idle": "2021-12-28T07:46:48.671925Z",
     "shell.execute_reply": "2021-12-28T07:46:48.670930Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1640375394900,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "7UU1VV5w2ESX",
    "outputId": "75b44382-5a20-4bc7-ddad-d4aaee8a218e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 6347703 chars\n",
      "ALLS WELL THAT ENDS WELL\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "Dramatis Personae\n",
      "\n",
      "  KING OF FRANCE\n",
      "  THE DUKE OF FLORENCE\n",
      "  BERTRAM, Count of Rousillon\n",
      "  LAFEU, an old lord\n",
      "  PAROLLES, a follower of Bertram\n",
      "  TWO FRENCH LORDS, serving with Bertram\n",
      "\n",
      "  STEWARD, Servant to the Countess of Rousillon\n",
      "  LAVACHE, a clown and Servant to the Countess of Rousillon\n",
      "  A PAGE, Servant to the Countess of Rousillon\n",
      "\n",
      "  COUNTESS OF ROUSILLON, mother to Bertram\n",
      "  HELENA, a gentlewoman protected by the Countess\n",
      "  A WIDOW OF FLORENCE.\n",
      "  DIANA, daughter to the Widow\n",
      "\n",
      "  VIOLENTA, neighbour and friend to the Widow\n",
      "  MARIANA, neighbour and friend to the Widow\n",
      "\n",
      "  Lords, Officers, Soldiers, etc., French and Florentine  \n",
      "\n",
      "SCENE:\n",
      "Rousillon; Paris; Florence; Marseilles\n",
      "\n",
      "ACT I. SCENE 1.\n",
      "Rousillon. The COUNT'S palace\n",
      "\n",
      "Enter BERTRAM, the COUNTESS OF ROUSILLON, HELENA, and LAFEU, all in black\n",
      "\n",
      "  COUNTESS. In delivering my son from me, I bury a second husband.\n",
      "  BERTRAM. And I in going, madam, weep o'er my father's death anew;\n",
      "    but I must attend his Majesty's command, to whom I am now in\n",
      "    ward, evermore in subjection.\n",
      "  LAFEU. You shall find of the King a husband, madam; you, sir, a\n",
      "    father. He that so generally is at all times good must of\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(f'Corpus length: {len(corpus)} chars')\n",
    "print(corpus[7:1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metoWpTc2ESY"
   },
   "source": [
    "### Data Preprocessing\n",
    "<a id=part1_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOR-1fZs2ESY"
   },
   "source": [
    "The first thing we'll need is to map from each unique character in the corpus to an index that will represent it in our learning process.\n",
    "\n",
    "**TODO**: Implement the `char_maps()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:48.676912Z",
     "iopub.status.busy": "2021-12-28T07:46:48.675914Z",
     "iopub.status.idle": "2021-12-28T07:46:48.890374Z",
     "shell.execute_reply": "2021-12-28T07:46:48.891400Z"
    },
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1640375396249,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "L4dM1FH42ESZ",
    "outputId": "2026462b-c3b9-4df4-cc8a-a9c9e2d00bf8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '[': 52, ']': 53, '_': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80, '}': 81, '\\ufeff': 82}\n"
     ]
    }
   ],
   "source": [
    "import hw3.charnn as charnn\n",
    "\n",
    "char_to_idx, idx_to_char = charnn.char_maps(corpus)\n",
    "print(char_to_idx)\n",
    "\n",
    "test.assertEqual(len(char_to_idx), len(idx_to_char))\n",
    "test.assertSequenceEqual(list(char_to_idx.keys()), list(idx_to_char.values()))\n",
    "test.assertSequenceEqual(list(char_to_idx.values()), list(idx_to_char.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4SqcZ922ESZ"
   },
   "source": [
    "Seems we have some strange characters in the corpus that are very rare and are probably due to mistakes.\n",
    "To reduce the length of each tensor we'll need to later represent our chars, it's best to remove them.\n",
    "\n",
    "**TODO**: Implement the `remove_chars()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:48.894364Z",
     "iopub.status.busy": "2021-12-28T07:46:48.894364Z",
     "iopub.status.idle": "2021-12-28T07:46:49.576333Z",
     "shell.execute_reply": "2021-12-28T07:46:49.577328Z"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1640375396725,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "3E1jrgAl2ESZ",
    "outputId": "f6a56dfa-8d5a-4ea3-fe9a-e4954d9f0fbe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 34 chars\n"
     ]
    }
   ],
   "source": [
    "corpus, n_removed = charnn.remove_chars(corpus, ['}','$','_','<','\\ufeff'])\n",
    "print(f'Removed {n_removed} chars')\n",
    "\n",
    "# After removing the chars, re-create the mappings\n",
    "char_to_idx, idx_to_char = charnn.char_maps(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxRHsM6f2ESa"
   },
   "source": [
    "The next thing we need is an **embedding** of the chracters.\n",
    "An embedding is a representation of each token from the sequence as a tensor.\n",
    "For a char-level RNN, our tokens will be chars and we can thus use the simplest possible embedding: encode each char as a **one-hot** tensor. In other words, each char will be represented\n",
    "as a tensor whos length is the total number of unique chars (`V`) which contains all zeros except at the index\n",
    "corresponding to that specific char.\n",
    "\n",
    "**TODO**: Implement the functions `chars_to_onehot()` and `onehot_to_chars()` in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:49.581317Z",
     "iopub.status.busy": "2021-12-28T07:46:49.581317Z",
     "iopub.status.idle": "2021-12-28T07:46:49.716529Z",
     "shell.execute_reply": "2021-12-28T07:46:49.716529Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1640375396726,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "-xJXGVtz2ESa",
    "outputId": "8f293f3f-8222-4130-e670-4b1f4b24bb8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brine a maiden can season her praise in.\n",
      "   \n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]], dtype=torch.int8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\DL class\\DL_class\\hw3\\hw3\\charnn.py:85: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  idx = ((embedded_text == 1).nonzero()[:, 1]).tolist()\n"
     ]
    }
   ],
   "source": [
    "# Wrap the actual embedding functions for calling convenience\n",
    "def embed(text):\n",
    "    return charnn.chars_to_onehot(text, char_to_idx)\n",
    "\n",
    "def unembed(embedding):\n",
    "    return charnn.onehot_to_chars(embedding, idx_to_char)\n",
    "\n",
    "text_snippet = corpus[3104:3148]\n",
    "print(text_snippet)\n",
    "print(embed(text_snippet[0:3]))\n",
    "\n",
    "test.assertEqual(text_snippet, unembed(embed(text_snippet)))\n",
    "test.assertEqual(embed(text_snippet).dtype, torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GalbvU-m2ESa"
   },
   "source": [
    "### Dataset Creation\n",
    "<a id=part1_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xE6O9TQ2ESb"
   },
   "source": [
    "We wish to train our model to generate text by constantly predicting what the next char should be based on the past.\n",
    "To that end we'll need to train our recurrent network in a way similar to a classification task. At each timestep, we input a char and set the expected output (label) to be the next char in the original sequence.\n",
    "\n",
    "We will split our corpus into shorter sequences of length `S` chars (see question below).\n",
    "Each **sample** we provide our model with will therefore be a tensor of shape `(S,V)` where `V` is the embedding dimension. Our model will operate sequentially on each char in the sequence.\n",
    "For each sample, we'll also need a **label**. This is simply another sequence, shifted by one char so that the label of each char is the next char in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlWrlqsb2ESb"
   },
   "source": [
    "**TODO**: Implement the `chars_to_labelled_samples()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:46:49.722511Z",
     "iopub.status.busy": "2021-12-28T07:46:49.722511Z",
     "iopub.status.idle": "2021-12-28T07:47:58.547305Z",
     "shell.execute_reply": "2021-12-28T07:47:58.546284Z"
    },
    "executionInfo": {
     "elapsed": 40008,
     "status": "ok",
     "timestamp": 1640375436728,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "-nA9oo142ESb",
    "outputId": "680432c0-d48f-4c0f-dc23-eeeaa73e0e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples shape: torch.Size([99182, 64, 78])\n",
      "labels shape: torch.Size([99182, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset of sequences\n",
    "seq_len = 64\n",
    "vocab_len = len(char_to_idx)\n",
    "# Create labelled samples\n",
    "samples, labels = charnn.chars_to_labelled_samples(corpus, char_to_idx, seq_len, device)\n",
    "print(f'samples shape: {samples.shape}')\n",
    "print(f'labels shape: {labels.shape}')\n",
    "\n",
    "# Test shapes\n",
    "num_samples = (len(corpus) - 1) // seq_len\n",
    "test.assertEqual(samples.shape, (num_samples, seq_len, vocab_len))\n",
    "test.assertEqual(labels.shape, (num_samples, seq_len))\n",
    "\n",
    "# Test content\n",
    "for r in range(1000):\n",
    "    # random sample\n",
    "    i = np.random.randint(num_samples, size=(1,))[0]\n",
    "    # Compare to corpus\n",
    "    test.assertEqual(unembed(samples[i]), corpus[i*seq_len:(i+1)*seq_len], msg=f\"content mismatch in sample {i}\")\n",
    "    # Compare to labels\n",
    "    sample_text = unembed(samples[i])\n",
    "    label_text = str.join('', [idx_to_char[j.item()] for j in labels[i]])\n",
    "    test.assertEqual(sample_text[1:], label_text[0:-1], msg=f\"label mismatch in sample {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dg29ioG12ESb"
   },
   "source": [
    "Let's print a few consecutive samples. You should see that the text continues between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:58.552268Z",
     "iopub.status.busy": "2021-12-28T07:47:58.551271Z",
     "iopub.status.idle": "2021-12-28T07:47:58.679920Z",
     "shell.execute_reply": "2021-12-28T07:47:58.679920Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1640375436728,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "VaRSefRM2ESc",
    "outputId": "3328f470-73f9-4c39-c387-1fde5465ed4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample [77436]:\n",
      "\te adieu. My father at the road Expects my coming, there to s\n",
      "sample [77437]:\n",
      "\tee me shipp'd. PROTEUS. And thither will I bring thee, Valenti\n",
      "sample [77438]:\n",
      "\tne. VALENTINE. Sweet Proteus, no; now let us take our leave.\n",
      "sample [77439]:\n",
      "\tTo Milan let me hear from thee by letters Of thy succes\n",
      "sample [77440]:\n",
      "\ts in love, and what news else Betideth here in absence of th\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "i = random.randrange(num_samples-5)\n",
    "for i in range(i, i+5):\n",
    "    test.assertEqual(len(samples[i]), seq_len)\n",
    "    s = re.sub(r'\\s+', ' ', unembed(samples[i])).strip()\n",
    "    print(f'sample [{i}]:\\n\\t{s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg1B7Fw-2ESc"
   },
   "source": [
    "As usual, instead of feeding one sample at a time into our model's forward we'll work with **batches** of samples. This means that at every timestep, our model will operate on a batch of chars that are from **different sequences**.\n",
    "Effectively this will allow us to parallelize training our model by dong matrix-matrix multiplications\n",
    "instead of matrix-vector during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HVFCXDw2ESc"
   },
   "source": [
    "An important nuance is that we need the batches to be **contiguous**, i.e. sample $k$ in batch $j$ should continue sample $k$ from batch $j-1$.\n",
    "The following figure illustrates this:\n",
    "\n",
    "<img src=\"imgs/rnn-batching.png\"/>\n",
    "\n",
    "If we na√Øvely take consecutive samples into batches, e.g. `[0,1,...,B-1]`, `[B,B+1,...,2B-1]` and so on, we won't have contiguous\n",
    "sequences at the same index between adjacent batches.\n",
    "\n",
    "To accomplish this we need to tell our `DataLoader` which samples to combine together into one batch.\n",
    "We do this by implementing a custom PyTorch `Sampler`, and providing it to our `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOlniQ6X2ESd"
   },
   "source": [
    "**TODO**: Implement the `SequenceBatchSampler` class in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:58.686929Z",
     "iopub.status.busy": "2021-12-28T07:47:58.683909Z",
     "iopub.status.idle": "2021-12-28T07:47:58.812680Z",
     "shell.execute_reply": "2021-12-28T07:47:58.812680Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1640375436729,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "iwNWHBFX2ESd",
    "outputId": "e1057c0b-2bcb-45ca-cfda-670b8fa79fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_idx =\n",
      " [tensor(0), tensor(1), tensor(2), tensor(3), tensor(4), tensor(5), tensor(6), tensor(7), tensor(8), tensor(9), tensor(10), tensor(11), tensor(12), tensor(13), tensor(14), tensor(15), tensor(16), tensor(17), tensor(18), tensor(19), tensor(20), tensor(21), tensor(22), tensor(23), tensor(24), tensor(25), tensor(26), tensor(27), tensor(28), tensor(29)]\n"
     ]
    }
   ],
   "source": [
    "from hw3.charnn import SequenceBatchSampler\n",
    "\n",
    "sampler = SequenceBatchSampler(dataset=range(32), batch_size=10)\n",
    "sampler_idx = list(sampler)\n",
    "print('sampler_idx =\\n', sampler_idx)\n",
    "\n",
    "# Test the Sampler\n",
    "test.assertEqual(len(sampler_idx), 30)\n",
    "batch_idx = np.array(sampler_idx).reshape(-1, 10)\n",
    "for k in range(10):\n",
    "    test.assertEqual(np.diff(batch_idx[:, k], n=2).item(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDLPf1G12ESd"
   },
   "source": [
    "Even though we're working with sequences, we can still use the standard PyTorch `Dataset`/`DataLoader` combo.\n",
    "For the dataset we can use a built-in class, `TensorDataset` to return tuples of `(sample, label)`\n",
    "from the `samples` and `labels` tensors we created above.\n",
    "The `DataLoader` will be provided with our custom `Sampler` so that it generates appropriate batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:58.817661Z",
     "iopub.status.busy": "2021-12-28T07:47:58.816662Z",
     "iopub.status.idle": "2021-12-28T07:47:58.945103Z",
     "shell.execute_reply": "2021-12-28T07:47:58.944104Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1640375436729,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "03m465lt2ESd"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "# Create DataLoader returning batches of samples.\n",
    "batch_size = 32\n",
    "\n",
    "ds_corpus = torch.utils.data.TensorDataset(samples, labels)\n",
    "sampler_corpus = SequenceBatchSampler(ds_corpus, batch_size)\n",
    "dl_corpus = torch.utils.data.DataLoader(ds_corpus, batch_size=batch_size, sampler=sampler_corpus, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYRYpx8x2ESd"
   },
   "source": [
    "Let's see what that gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:58.949089Z",
     "iopub.status.busy": "2021-12-28T07:47:58.948091Z",
     "iopub.status.idle": "2021-12-28T07:47:59.085723Z",
     "shell.execute_reply": "2021-12-28T07:47:59.086721Z"
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1640375437221,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "1BMd8Ivz2ESd",
    "outputId": "f0a017c2-e2c3-455e-fe0b-1ad7abc39a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batches: 3100\n",
      "shape of a batch of samples: torch.Size([32, 64, 78])\n",
      "shape of a batch of labels: torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'num batches: {len(dl_corpus)}')\n",
    "\n",
    "x0, y0 = next(iter(dl_corpus))\n",
    "print(f'shape of a batch of samples: {x0.shape}')\n",
    "print(f'shape of a batch of labels: {y0.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VH04gcs2ESe"
   },
   "source": [
    "Now lets look at the same sample index from multiple batches taken from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:59.091708Z",
     "iopub.status.busy": "2021-12-28T07:47:59.091708Z",
     "iopub.status.idle": "2021-12-28T07:47:59.233330Z",
     "shell.execute_reply": "2021-12-28T07:47:59.234326Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1640375437222,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "ahyI9ghc2ESe",
    "outputId": "b9ee64ee-b532-474c-8d42-435c470b6d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== batch 0, sample 26 (torch.Size([64, 78])): ===\n",
      "\tan had a father- O, that 'had,' how sad a passage 'tis!-whos\n",
      "=== batch 1, sample 26 (torch.Size([64, 78])): ===\n",
      "\tCOUNTESS. Be thou blest, Bertram, and succeed thy father In\n",
      "=== batch 2, sample 26 (torch.Size([64, 78])): ===\n",
      "\tt I know him a notorious liar, Think him a great way fool, s\n",
      "=== batch 3, sample 26 (torch.Size([64, 78])): ===\n",
      "\tfeeding his own stomach. Besides, virginity is peevish, proud,\n",
      "=== batch 4, sample 26 (torch.Size([64, 78])): ===\n",
      "\tEnter PAGE PAGE. Monsieur Parolles, my lo\n"
     ]
    }
   ],
   "source": [
    "# Check that sentences in in same index of different batches complete each other.\n",
    "k = random.randrange(batch_size)\n",
    "for j, (X, y) in enumerate(dl_corpus,):\n",
    "    print(f'=== batch {j}, sample {k} ({X[k].shape}): ===')\n",
    "    s = re.sub(r'\\s+', ' ', unembed(X[k])).strip()\n",
    "    print(f'\\t{s}')\n",
    "    if j==4: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ibxOLsm2ESe"
   },
   "source": [
    "### Model Implementation\n",
    "<a id=part1_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEbjmLXk2ESe"
   },
   "source": [
    "Finally, our data set is ready so we can focus on our model.\n",
    "\n",
    "We'll implement here is a multilayer gated recurrent unit (GRU) model, with dropout.\n",
    "This model is a type of RNN which performs similar to the well-known LSTM model,\n",
    "but it's somewhat easier to train because it has less parameters.\n",
    "We'll modify the regular GRU slightly by applying dropout to\n",
    "the hidden states passed between layers of the model.\n",
    "\n",
    "The model accepts an input $\\mat{X}\\in\\set{R}^{S\\times V}$ containing a sequence of embedded chars.\n",
    "It returns an output $\\mat{Y}\\in\\set{R}^{S\\times V}$ of predictions for the next char and the final hidden state\n",
    "$\\mat{H}\\in\\set{R}^{L\\times H}$. Here $S$ is the sequence length, $V$ is the vocabulary size (number of unique chars), $L$ is the number of layers in the model and $H$ is the hidden dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBVTEWvh2ESe"
   },
   "source": [
    "Mathematically, the model's forward function at layer $k\\in[1,L]$ and timestep $t\\in[1,S]$ can be described as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{z_t}^{[k]} &= \\sigma\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xz}}}^{[k]} +\n",
    "    \\vec{h}_{t-1}^{[k]} {\\mattr{W}_{\\mathrm{hz}}}^{[k]} + \\vec{b}_{\\mathrm{z}}^{[k]}\\right) \\\\\n",
    "\\vec{r_t}^{[k]} &= \\sigma\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xr}}}^{[k]} +\n",
    "    \\vec{h}_{t-1}^{[k]} {\\mattr{W}_{\\mathrm{hr}}}^{[k]} + \\vec{b}_{\\mathrm{r}}^{[k]}\\right) \\\\\n",
    "\\vec{g_t}^{[k]} &= \\tanh\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xg}}}^{[k]} +\n",
    "    (\\vec{r_t}^{[k]}\\odot\\vec{h}_{t-1}^{[k]}) {\\mattr{W}_{\\mathrm{hg}}}^{[k]} + \\vec{b}_{\\mathrm{g}}^{[k]}\\right) \\\\\n",
    "\\vec{h_t}^{[k]} &= \\vec{z}^{[k]}_t \\odot \\vec{h}^{[k]}_{t-1} + \\left(1-\\vec{z}^{[k]}_t\\right)\\odot \\vec{g_t}^{[k]}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgy5ogvx2ESf"
   },
   "source": [
    "The input to each layer is,\n",
    "$$\n",
    "\\mat{X}^{[k]} =\n",
    "\\begin{bmatrix}\n",
    "    {\\vec{x}_1}^{[k]} \\\\ \\vdots \\\\ {\\vec{x}_S}^{[k]}\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{cases}\n",
    "    \\mat{X} & \\mathrm{if} ~k = 1~ \\\\\n",
    "    \\mathrm{dropout}_p \\left(\n",
    "    \\begin{bmatrix}\n",
    "        {\\vec{h}_1}^{[k-1]} \\\\ \\vdots \\\\ {\\vec{h}_S}^{[k-1]}\n",
    "    \\end{bmatrix} \\right) & \\mathrm{if} ~1 < k \\leq L+1~\n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqMglGwi2ESf"
   },
   "source": [
    "The output of the entire model is then,\n",
    "$$\n",
    "\\mat{Y} = \\mat{X}^{[L+1]} {\\mattr{W}_{\\mathrm{hy}}} + \\mat{B}_{\\mathrm{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IqUiaD42ESf"
   },
   "source": [
    "and the final hidden state is\n",
    "$$\n",
    "\\mat{H} = \n",
    "\\begin{bmatrix}\n",
    "    {\\vec{h}_S}^{[1]} \\\\ \\vdots \\\\ {\\vec{h}_S}^{[L]}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWqZ2VzO2ESf"
   },
   "source": [
    "Notes:\n",
    "- $t\\in[1,S]$ is the timestep, i.e. the current position within the sequence of each sample.\n",
    "- $\\vec{x}_t^{[k]}$ is the input of layer $k$ at timestep $t$, respectively.\n",
    "- The outputs of the **last layer** $\\vec{y}_t^{[L]}$, are the predicted next characters for every input char.\n",
    "  These are similar to class scores in classification tasks.\n",
    "- The hidden states at the **last timestep**, $\\vec{h}_S^{[k]}$, are the final hidden state returned from the model.\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function, i.e. $\\sigma(\\vec{z}) = 1/(1+e^{-\\vec{z}})$ which returns values in $(0,1)$.\n",
    "- $\\tanh(\\cdot)$ is the hyperbolic tangent, i.e. $\\tanh(\\vec{z}) = (e^{2\\vec{z}}-1)/(e^{2\\vec{z}}+1)$ which returns values in $(-1,1)$.\n",
    "- $\\vec{h_t}^{[k]}$ is the hidden state of layer $k$ at time $t$. This can be thought of as the memory of that layer.\n",
    "- $\\vec{g_t}^{[k]}$ is the candidate hidden state for time $t+1$.\n",
    "- $\\vec{z_t}^{[k]}$ is known as the update gate. It combines the previous state with the input to determine how much the current state will be combined with the new candidate state. For example, if $\\vec{z_t}^{[k]}=\\vec{1}$ then the current input has no effect on the output.\n",
    "- $\\vec{r_t}^{[k]}$ is known as the reset gate. It combines the previous state with the input to determine how much of the previous state will affect the current state candidate. For example if $\\vec{r_t}^{[k]}=\\vec{0}$ the previous state has no effect on the current candidate state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGrsGkWS2ESf"
   },
   "source": [
    "Here's a graphical representation of the GRU's forward pass at each timestep. The $\\vec{\\tilde{h}}$ in the image is our $\\vec{g}$ (candidate next state).\n",
    "\n",
    "<img src=\"imgs/gru_cell.png\" width=\"400\"/>\n",
    "\n",
    "You can see how the reset and update gates allow the model to completely ignore it's previous state, completely ignore it's input, or any mixture of those states (since the gates are actually continuous and between $(0,1)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnSuFMOc2ESf"
   },
   "source": [
    "Here's a graphical representation of the entire model.\n",
    "You can ignore the $c_t^{[k]}$ (cell state) variables (which are relevant for LSTM models).\n",
    "Our model has only the hidden state, $h_t^{[k]}$. Also notice that we added dropout between layers (i.e., on the up arrows).\n",
    "\n",
    "<img src=\"imgs/lstm_model.png\" />\n",
    "\n",
    "The purple tensors are inputs (a sequence and initial hidden state per layer), and the green tensors are outputs (another sequence and final hidden state per layer). Each blue block implements the above forward equations.\n",
    "Blocks that are on the same vertical level are at the same layer, and therefore share parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt5ZRNAe2ESg"
   },
   "source": [
    "**TODO**: Implement the `MultilayerGRU` class in the `hw3/charnn.py` module.\n",
    "\n",
    "Notes:\n",
    "- You'll need to handle input **batches** now.\n",
    "  The math is identical to the above, but all the tensors will have an extra batch\n",
    "  dimension as their first dimension.\n",
    "- Use the diagram above to help guide your implementation.\n",
    "  It will help you visualize what shapes to returns where, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:59.244300Z",
     "iopub.status.busy": "2021-12-28T07:47:59.243303Z",
     "iopub.status.idle": "2021-12-28T07:47:59.633765Z",
     "shell.execute_reply": "2021-12-28T07:47:59.634764Z"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1640375437617,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "iIlWe9YG2ESg",
    "outputId": "54a1c88e-d932-4b54-9662-4cff6409ea15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerGRU(\n",
      "  (xz_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (hz_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xr_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (hr_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xg_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (hg_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xz_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hz_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xr_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hr_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xg_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hg_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xz_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hz_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xr_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hr_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xg_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hg_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=78, bias=True)\n",
      ")\n",
      "x0 shape torch.Size([32, 64, 78])\n",
      "torch.Size([32, 64, 78])\n",
      "y.shape=torch.Size([32, 64, 78])\n",
      "h.shape=torch.Size([32, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "in_dim = vocab_len\n",
    "h_dim = 256\n",
    "n_layers = 3\n",
    "model = charnn.MultilayerGRU(in_dim, h_dim, out_dim=in_dim, n_layers=n_layers)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "print(f'x0 shape {x0.shape}')\n",
    "y, h = model(x0.to(dtype=torch.float, device=device))\n",
    "print(y.shape)\n",
    "print(f'y.shape={y.shape}')\n",
    "print(f'h.shape={h.shape}')\n",
    "\n",
    "test.assertEqual(y.shape, (batch_size, seq_len, vocab_len))\n",
    "test.assertEqual(h.shape, (batch_size, n_layers, h_dim))\n",
    "test.assertEqual(len(list(model.parameters())), 9 * n_layers + 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpftsW5J2ESg"
   },
   "source": [
    "### Generating text by sampling\n",
    "<a id=part1_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt_FzgNl2ESg"
   },
   "source": [
    "Now that we have a model, we can implement **text generation** based on it.\n",
    "The idea is simple:\n",
    "At each timestep our model receives one char $x_t$ from the input sequence and outputs scores $y_t$\n",
    "for what the next char should be.\n",
    "We'll convert these scores into a probability over each of the possible chars.\n",
    "In other words, for each input char $x_t$ we create a probability distribution for the next char\n",
    "conditioned on the current one and the state of the model (representing all previous inputs):\n",
    "$$p(x_{t+1}|x_t, \\vec{h}_t).$$\n",
    "\n",
    "Once we have such a distribution, we'll sample a char from it.\n",
    "This will be the first char of our generated sequence.\n",
    "Now we can feed this new char into the model, create another distribution, sample the next char and so on.\n",
    "Note that it's crucial to propagate the hidden state when sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkrvQyy22ESg"
   },
   "source": [
    "The important point however is how to create the distribution from the scores.\n",
    "One way, as we saw in previous ML tasks, is to use the softmax function.\n",
    "However, a drawback of softmax is that it can generate very diffuse (more uniform) distributions if the score values are very similar. When sampling, we would prefer to control the distributions and make them less uniform to increase the chance of sampling the char(s) with the highest scores compared to the others.\n",
    "\n",
    "To control the variance of the distribution, a common trick is to add a hyperparameter $T$, known as the \n",
    "*temperature* to the softmax function. The class scores are simply scaled by $T$ before softmax is applied:\n",
    "$$\n",
    "\\mathrm{softmax}_T(\\vec{y}) = \\frac{e^{\\vec{y}/T}}{\\sum_k e^{y_k/T}}\n",
    "$$\n",
    "\n",
    "A low $T$ will result in less uniform distributions and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYphobrb2ESg"
   },
   "source": [
    "**TODO**: Implement the `hot_softmax()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:59.641744Z",
     "iopub.status.busy": "2021-12-28T07:47:59.641744Z",
     "iopub.status.idle": "2021-12-28T07:47:59.821264Z",
     "shell.execute_reply": "2021-12-28T07:47:59.821264Z"
    },
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1640375438459,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "lVfef9ED2ESh",
    "outputId": "badc3024-3f2f-483b-9265-8bcfdb674124"
   },
   "outputs": [],
   "source": [
    "scores = y[0,0,:].detach()\n",
    "_, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "for t in reversed([0.3, 0.5, 1.0, 100]):\n",
    "    ax.plot(charnn.hot_softmax(scores, temperature=t).cpu().numpy(), label=f'T={t}')\n",
    "ax.set_xlabel('$x_{t+1}$')\n",
    "ax.set_ylabel('$p(x_{t+1}|x_t)$')\n",
    "ax.legend()\n",
    "\n",
    "uniform_proba = 1/len(char_to_idx)\n",
    "uniform_diff = torch.abs(charnn.hot_softmax(scores, temperature=100) - uniform_proba)\n",
    "test.assertTrue(torch.all(uniform_diff < 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTFyZmqP2ESh"
   },
   "source": [
    "**TODO**: Implement the `generate_from_model()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:47:59.825253Z",
     "iopub.status.busy": "2021-12-28T07:47:59.825253Z",
     "iopub.status.idle": "2021-12-28T07:48:00.126292Z",
     "shell.execute_reply": "2021-12-28T07:48:00.127288Z"
    },
    "executionInfo": {
     "elapsed": 5282,
     "status": "ok",
     "timestamp": 1640375443736,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "Z8kt7LyW2ESh",
    "outputId": "17a43904-735e-4e21-e664-a8af19d075f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobarn'hetDd3NQV'5;VIH.fzGqJk\n",
      "mfJWb'DXklZUYd-pHDd\n",
      "foobar9;LFu:TeUtZeZL)U12a&Ru)m3EwXWDL\"tYqJWa7n( w&\n",
      "foobarCJgf\"QcmiY(\n",
      "aSF6Ay3yKA[MFQ3'UPMyz4d0V;.YN\n",
      "z3\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    text = charnn.generate_from_model(model, \"foobar\", 50, (char_to_idx, idx_to_char), T=0.5)\n",
    "    print(text)\n",
    "\n",
    "    test.assertEqual(len(text), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghjts99C2ESh"
   },
   "source": [
    "### Training\n",
    "<a id=part1_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmz0ofaS2ESh"
   },
   "source": [
    "To train this model, we'll calculate the loss at each time step by comparing the predicted char to\n",
    "the actual char from our label. We can use cross entropy since per char it's similar to a classification problem.\n",
    "We'll then sum the losses over the sequence and back-propagate the gradients though time.\n",
    "Notice that the back-propagation algorithm will \"visit\" each layer's parameter tensors multiple times,\n",
    "so we'll accumulate gradients in parameters of the blocks. Luckily `autograd` will handle this part for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb5HHYo92ESh"
   },
   "source": [
    "As usual, the first step of training will be to try and **overfit** a large model (many parameters) to a tiny dataset.\n",
    "Again, this is to ensure the model and training code are implemented correctly, i.e. that the model can learn.\n",
    "\n",
    "For a generative model such as this, overfitting is slightly trickier than for classification.\n",
    "What we'll aim to do is to get our model to **memorize** a specific sequence of chars, so that when given the first\n",
    "char in the sequence it will immediately spit out the rest of the sequence verbatim.\n",
    "\n",
    "Let's create a tiny dataset to memorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:48:00.134269Z",
     "iopub.status.busy": "2021-12-28T07:48:00.133272Z",
     "iopub.status.idle": "2021-12-28T07:48:00.289853Z",
     "shell.execute_reply": "2021-12-28T07:48:00.290851Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1640375443737,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "XzgjM18i2ESh",
    "outputId": "e8df18e4-821e-4808-a561-303cc9c9ea21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to \"memorize\":\n",
      "\n",
      "TRAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not tell you what I would, my lord.\n",
      "    Faith, yes:\n",
      "    Strangers and foes do sunder and not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HE\n"
     ]
    }
   ],
   "source": [
    "# Pick a tiny subset of the dataset\n",
    "subset_start, subset_end = 1001, 1005\n",
    "ds_corpus_ss = torch.utils.data.Subset(ds_corpus, range(subset_start, subset_end))\n",
    "batch_size_ss = 1\n",
    "sampler_ss = SequenceBatchSampler(ds_corpus_ss, batch_size=batch_size_ss)\n",
    "dl_corpus_ss = torch.utils.data.DataLoader(ds_corpus_ss, batch_size_ss, sampler=sampler_ss, shuffle=False)\n",
    "\n",
    "# Convert subset to text\n",
    "subset_text = ''\n",
    "for i in range(subset_end - subset_start):\n",
    "    subset_text += unembed(ds_corpus_ss[i][0])\n",
    "print(f'Text to \"memorize\":\\n\\n{subset_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii6yQRhA2ESi"
   },
   "source": [
    "Now let's implement the first part of our training code.\n",
    "\n",
    "**TODO**: Implement the `train_epoch()` and `train_batch()` methods of the `RNNTrainer` class in the `hw3/training.py` module. \n",
    "You must think about how to correctly handle the hidden state of the model between batches and epochs for this specific task (i.e. text generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:48:00.299828Z",
     "iopub.status.busy": "2021-12-28T07:48:00.298830Z",
     "iopub.status.idle": "2021-12-28T07:48:24.076314Z",
     "shell.execute_reply": "2021-12-28T07:48:24.076314Z"
    },
    "executionInfo": {
     "elapsed": 136685,
     "status": "ok",
     "timestamp": 1640375580409,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "w9doxEJf2ESi",
    "outputId": "9584ce6f-3b8f-4455-ac80-b710718bba57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not tell you what I would, my lord.\n",
      "    Faith, yes:\n",
      "    Strangers and foes do sunder and not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\DL class\\DL_class\\hw3\\hw3\\training.py:273: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  num_correct = torch.sum(torch.tensor(pred.argmax(dim=1).eq(y)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #1: Avg. loss = 3.940, Accuracy = 17.58%\n",
      "Tt                                                                                                                                                                                                                                           o                  \n",
      "\n",
      "Epoch #25: Avg. loss = 0.268, Accuracy = 96.09%\n",
      "TAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not would you wand not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HELERAM. I pray you, stay not, but in haste to horse.\n",
      "  HELERAM. I pray y\n",
      "\n",
      "Epoch #50: Avg. loss = 0.008, Accuracy = 100.00%\n",
      "TRAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not tell you what I would, my lord.\n",
      "    Faith, yes:\n",
      "    Strangers and foes do sunder and not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HE\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from hw3.training import RNNTrainer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 500\n",
    "\n",
    "in_dim = vocab_len\n",
    "h_dim = 128\n",
    "n_layers = 2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = charnn.MultilayerGRU(in_dim, h_dim, out_dim=in_dim, n_layers=n_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = RNNTrainer(model, loss_fn, optimizer, device)\n",
    "print(subset_text)\n",
    "for epoch in range(num_epochs):\n",
    "    # print(dl_corpus_ss[0])\n",
    "    epoch_result = trainer.train_epoch(dl_corpus_ss, verbose=False)\n",
    "    # print('done')\n",
    "    # Every X epochs, we'll generate a sequence starting from the first char in the first sequence\n",
    "    # to visualize how/if/what the model is learning.\n",
    "    if epoch == 0 or (epoch+1) % 25 == 0:\n",
    "        avg_loss = np.mean(epoch_result.losses)\n",
    "        accuracy = np.mean(epoch_result.accuracy)\n",
    "        print(f'\\nEpoch #{epoch+1}: Avg. loss = {avg_loss:.3f}, Accuracy = {accuracy:.2f}%')\n",
    "        \n",
    "        generated_sequence = charnn.generate_from_model(model, subset_text[0],\n",
    "                                                        seq_len*(subset_end-subset_start),\n",
    "                                                        (char_to_idx,idx_to_char), T=0.1)\n",
    "        \n",
    "        # Stop if we've successfully memorized the small dataset.\n",
    "        print(generated_sequence)\n",
    "        if generated_sequence == subset_text:\n",
    "            break\n",
    "\n",
    "# Test successful overfitting\n",
    "test.assertGreater(epoch_result.accuracy, 99)\n",
    "test.assertEqual(generated_sequence, subset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piw96c6U2ESi"
   },
   "source": [
    "OK, so training works - we can memorize a short sequence.\n",
    "We'll now train a much larger model on our large dataset. You'll need a GPU for this part.\n",
    "\n",
    "First, lets set up our dataset and models for training.\n",
    "We'll split our corpus into 90% train and 10% test-set.\n",
    "Also, we'll use a learning-rate scheduler to control the learning rate during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1QfJMHk2ESi"
   },
   "source": [
    "**TODO**: Set the hyperparameters in the `part1_rnn_hyperparams()` function of the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:48:24.084293Z",
     "iopub.status.busy": "2021-12-28T07:48:24.083296Z",
     "iopub.status.idle": "2021-12-28T07:49:07.874398Z",
     "shell.execute_reply": "2021-12-28T07:49:07.873399Z"
    },
    "executionInfo": {
     "elapsed": 27497,
     "status": "ok",
     "timestamp": 1640375607903,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "P1CtiWiG2ESi",
    "outputId": "dd7cc445-fcac-4252-eb9b-836434ae3cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparams:\n",
      " {'batch_size': 128, 'seq_len': 60, 'h_dim': 256, 'n_layers': 3, 'dropout': 0.25, 'learn_rate': 0.0005, 'lr_sched_factor': 0.23, 'lr_sched_patience': 4}\n",
      "Train: 743 batches, 5706240 chars\n",
      "Test:   82 batches,  629760 chars\n"
     ]
    }
   ],
   "source": [
    "from hw3.answers import part1_rnn_hyperparams\n",
    "\n",
    "hp = part1_rnn_hyperparams()\n",
    "print('hyperparams:\\n', hp)\n",
    "### Dataset definition\n",
    "vocab_len = len(char_to_idx)\n",
    "batch_size = hp['batch_size']\n",
    "seq_len = hp['seq_len']\n",
    "train_test_ratio = 0.9\n",
    "num_samples = (len(corpus) - 1) // seq_len\n",
    "num_train = int(train_test_ratio * num_samples)\n",
    "\n",
    "samples, labels = charnn.chars_to_labelled_samples(corpus, char_to_idx, seq_len, device)\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(samples[:num_train], labels[:num_train])\n",
    "sampler_train = SequenceBatchSampler(ds_train, batch_size)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False, sampler=sampler_train, drop_last=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(samples[num_train:], labels[num_train:])\n",
    "sampler_test = SequenceBatchSampler(ds_test, batch_size)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=False, sampler=sampler_test, drop_last=True)\n",
    "\n",
    "print(f'Train: {len(dl_train):3d} batches, {len(dl_train)*batch_size*seq_len:7d} chars')\n",
    "print(f'Test:  {len(dl_test):3d} batches, {len(dl_test)*batch_size*seq_len:7d} chars')\n",
    "\n",
    "### Training definition\n",
    "in_dim = out_dim = vocab_len\n",
    "checkpoint_file = 'checkpoints/final_best.pt'\n",
    "num_epochs = 50\n",
    "early_stopping = 5\n",
    "\n",
    "model = charnn.MultilayerGRU(in_dim, hp['h_dim'], out_dim, hp['n_layers'], hp['dropout'])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp['learn_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=hp['lr_sched_factor'], patience=hp['lr_sched_patience'], verbose=True\n",
    ")\n",
    "trainer = RNNTrainer(model, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h89GEiKj2ESj"
   },
   "source": [
    "The code blocks below will train the model and save checkpoints containing the training state and the best model parameters to a file. This allows you to stop training and resume it later from where you left.\n",
    "\n",
    "Note that you can use the `main.py` script provided within the assignment folder to run this notebook from the command line as if it were a python script by using the `run-nb` subcommand. This allows you to train your model using this notebook without starting jupyter. You can combine this with `srun` or `sbatch` to run the notebook with a GPU on the course servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USMBGlEA2ESj"
   },
   "source": [
    "**TODO**:\n",
    "- Implement the `fit()` method of the `Trainer` class. You can reuse the relevant implementation parts from HW2, but make sure to implement early stopping and checkpoints.\n",
    "- Implement the `test_epoch()` and `test_batch()` methods of the `RNNTrainer` class in the `hw3/training.py` module.\n",
    "- Run the following block to train.\n",
    "- When training is done and you're satisfied with the model's outputs, rename the checkpoint file to `checkpoints/rnn_final.pt`.\n",
    "  This will cause the block to skip training and instead load your saved model when running the homework submission script.\n",
    "  Note that your submission zip file will not include the checkpoint file. This is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:07.881378Z",
     "iopub.status.busy": "2021-12-28T07:49:07.880381Z",
     "iopub.status.idle": "2021-12-28T07:49:08.051922Z",
     "shell.execute_reply": "2021-12-28T07:49:08.052920Z"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1640375608733,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "Om6mCxKN2ESj",
    "outputId": "3b951785-d4ec-40cf-a9cb-ff3e8ca8db56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading final checkpoint file checkpoints/final_best.pt instead of training\n"
     ]
    }
   ],
   "source": [
    "from cs3600.plot import plot_fit\n",
    "\n",
    "def post_epoch_fn(epoch, train_res, test_res, verbose):\n",
    "    # Update learning rate\n",
    "    scheduler.step(test_res.accuracy)\n",
    "    # Sample from model to show progress\n",
    "    if verbose:\n",
    "        start_seq = \"ACT I.\"\n",
    "        generated_sequence = charnn.generate_from_model(\n",
    "            model, start_seq, 100, (char_to_idx,idx_to_char), T=0.5\n",
    "        )\n",
    "        print(generated_sequence)\n",
    "\n",
    "# Train, unless final checkpoint is found\n",
    "checkpoint_file_final = checkpoint_file\n",
    "\n",
    "\n",
    "if os.path.isfile(checkpoint_file_final):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    saved_state = torch.load(checkpoint_file_final, map_location=device)\n",
    "    model.load_state_dict(saved_state['model_state'])\n",
    "else:\n",
    "  try:\n",
    "      # Print pre-training sampling\n",
    "      print(charnn.generate_from_model(model, \"ACT I.\", 100, (char_to_idx,idx_to_char), T=0.5))\n",
    "\n",
    "      fit_res = trainer.fit(dl_train, dl_test, 50, max_batches=None,\n",
    "                            post_epoch_fn=post_epoch_fn, early_stopping=early_stopping,\n",
    "                            checkpoints=checkpoint_file, print_every=1)\n",
    "      \n",
    "      fig, axes = plot_fit(fit_res)\n",
    "  except KeyboardInterrupt as e:\n",
    "      print('\\n *** Training interrupted by user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlUXj-xn2ESj"
   },
   "source": [
    "### Generating a work of art\n",
    "<a id=part1_8></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMFgvWfC2ESj"
   },
   "source": [
    "Armed with our fully trained model, let's generate the next Hamlet! You should experiment with modifying the sampling temperature and see what happens.\n",
    "\n",
    "The text you generate should ‚Äúlook‚Äù like a Shakespeare play:\n",
    "old-style English words and sentence structure, directions for the actors\n",
    "(like ‚ÄúExit/Enter‚Äù), sections (Act I/Scene III) etc.\n",
    "There will be no coherent plot of course, but it should at least seem like\n",
    "a Shakespearean play when not looking too closely.\n",
    "If this is not what you see, go back, debug and/or and re-train.\n",
    "\n",
    "**TODO**: Specify the generation parameters in the `part1_generation_params()` function within the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:08.058904Z",
     "iopub.status.busy": "2021-12-28T07:49:08.057907Z",
     "iopub.status.idle": "2021-12-28T07:49:23.459926Z",
     "shell.execute_reply": "2021-12-28T07:49:23.459926Z"
    },
    "id": "a8_Ddst62ESk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the world's a stage the tian the sea \n",
      "\n",
      "And with a trie of the chrelews are th the weetd of the oavnd,\n",
      "And this not an  his ciles and this repair the land of secvice \n",
      "And sha  the winter of the ceart bfe sweetest king on thi most pense, and the world \n",
      "\n",
      "KING RICHARD II:\n",
      "What are  my lord? the answer sno the world.\n",
      "\n",
      "SICINIUS:\n",
      "Ihat is the graae of the soall thou art \n",
      "\n",
      "h  it that s tor  and makes ae seen tis  themp \n",
      "t the ferest\n",
      "To bive the king of tour sigher of the battle oall.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, what s then dod here with the state \n",
      "Still not so e three of this tete that tay  o wear the suke\n",
      "of the heart.\n",
      "\n",
      "LADY BOCERON:\n",
      "Wiat,that shall I sae the ?\n",
      "\n",
      "QUEEN:\n",
      "The sord of his hons that in eerots the weary of hoaden fore hem heart,\n",
      "    Whin I am a l the state of the tanter of the shame\n",
      "A d sound the mae the caue and princ oo t me my nacerof hiur and there te seen the r son\n",
      "Th wee the reat  for what they ase to sea the world,\n",
      "And tame to tee comrt oin the thms and said toeir hands\n",
      "\n",
      "KING RICHARD III:\n",
      "Master Senate  burn in him all to tee the comalis for his faot.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Wo Ie werl be ahe bosom of the father s lore.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ohet hhou dost tot hhou thet a wood deatom that he thke the happy day of monster?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Belide him heth he na motr thang be a r te ns \n",
      "Th go arr the lare  tf his trngues ane ahe dores.\n",
      "\n",
      "DUCE OF YORK:\n",
      "For theu htst is the state of whom I sae \n",
      "MnUEEN ELIZABETH:\n",
      "Ithave me fool'd in the dorl.r spint of wordd\n",
      "\n",
      "\n",
      "KING RICHARD II:\n",
      "Ahe  wall thou dost thou th the trath?\n",
      "\n",
      "BUNGOR:\n",
      "And thon thes is the world tha pronaon oortuour coown\n",
      "The  ing  the man is the fair so san \n",
      "Whth hitter anm er and wi the common time,\n",
      "And now he will sh shall te are ale  oe  the pronce of maney day,\n",
      "That we may tor t to the winh  of moneue wf his nime.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ihe eare of tolinge that I ll see the earth of daneer  or note tor my sorved grane,\n",
      "Whose sturt to the liast  that ch t woll profe to  nd \n",
      "Bhe e hhou the elone oose twhce make aine house ind wake desire!\n",
      "But torl te sound the parn of the fool \n",
      "\n",
      "DUKE OF YORK:\n",
      "Here, shr, bfr short that hane the state of your solvest manner,\n",
      "Tnou mort rebe a wrong  and his rest.\n",
      "\n",
      "BUCKINGHAM:\n",
      "The contract  the  have sent tnmthe poistaos \n",
      "The prince tf defd of nite  thou hast ao the thy house is soch and ains.\n",
      "\n",
      "PLADCine of this tenple thabe a wrrld,\n",
      "Aod bh with his wight the cather of the propd aneeki trom the  werp \n",
      "s,\n",
      "That all the e tur eyes ahese mine is the lang,\n",
      "Thd then tho  ta the bits and the bing of loe dear lores and stowt to have mou to his \n",
      "eat wad heard \n",
      "and wom  e thoughts \n",
      "  HERMIONL:\n",
      "Ohat was a state of thn the fare er wo the moreife.\n",
      "\n",
      "KUCY:\n",
      "Ay, byt tho wile of suare\n",
      "\n",
      "KANGERELE:\n",
      "No more the grave oo h ties, the company,\n",
      "g in the lart. and tear my soul,\n",
      "Our pords fd country was the rest,\n",
      "Where wolt be sather' tere so fith the steel of that \n",
      "\n",
      "nd soul ha would I henr the  the head of hhe shape ofe maneer that the king should be stranghd  the searts with his form wh the mand of thie are the mother's sonl tn some aoye,\n",
      "That s is a song of your lady to the  stand, flr hhe stows tot gay ao mis sou\n",
      "a sloging and to this light:\n",
      "O  when he ware dead  the  repent  the warth of tny of the .\n",
      "\n",
      "DUKE OF YORK:\n",
      "I  tour winds, ae were a sorth, and not the besrt\n",
      "The oather of the matter of the world heth deneed the fatr consand, waid  the other Thou ahink it that \n",
      " weten ome sn e tn  love s for the staths of the first of else.teaves and sare and peth o with my favour,\n",
      "The shall not be me th the striet of the gant \n",
      "\n",
      "Sirst thou are come tath a  present sor the company,\n",
      "Dnd with the ping of our bong iresent treaence the world,\n",
      "The world wee we shall se e our drace to t he will be feard to mou tor this king-\n",
      "Thou hast remember to te grant the woll,\n",
      "\n",
      "ith the tirough of the street oo the wall,\n",
      "The state of fellowsshat the deail \n",
      "Ahe  will not se dtand the commort the  with a ting.\n",
      "\n",
      "y gord, thou sealt aot speak the warter of the country,\n",
      "And strumpet that you shall be so speed  to be the bali and ahe clotk,\n",
      "Which sol the world is to the good tha way, \n",
      "QRCONS II:\n",
      "A d th t the  cell  he  the courtry of thesfather's uest.\n",
      "\n",
      "DUKE OF YLOXENES:\n",
      "Why, the  hons not to me to the raster  and him name ws not to see,\n",
      "Th we tave a de and seev the  lain of your ard to he all the song.\n",
      "\n",
      "ou the   the worst to the closet of mhe woetd;\n",
      "  FROTEUS:\n",
      "You  aar she sui oi the noart  of wreser to kois deaedient han.\n",
      "\n",
      "HORG RICH:\n",
      "What shy that thou shalet hhe prisees of tou sondier\n",
      "T  tand with the mord of the heart \n",
      "\n",
      "Sorvet ahe weater with the childrof all the truie,\n",
      "Hro  thi mord  that fhou wive thee desperate te the war.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I will not little she he prove the wer \n",
      "Are heaven wis hoart to bear the world,\n",
      "What saee  and to make me son and so prrson to the r lown \n",
      "\n",
      "Ahe giod gracious sovereign be os,\n",
      "\n",
      "GLOUCESTER:\n",
      "I know not me, who te  so weeat the suit\n",
      "That t woth tye villains of the gang of consent \n",
      "T strange the born and tage \n",
      "The  honest wind to the gay of the world\n",
      "I have the world, to do the crowns and tade the poulon from the .\n",
      "\n",
      "BUCKINGHAM:\n",
      "How now, fy lord,\n",
      "and di my soul on tour feart\n",
      "And ho the seatt of tree of tring of men \n",
      "\n",
      "here ao the man er of tour stn witl blood io tour the King,\n",
      "before your latcher , and the pownt of the koese of youe with me,\n",
      "Wnd stand to the wonse of thll of him, and the forl of more,\n",
      "With deae ng with the way on the daeat of the field, And mill the right of his words  fnd tanish in the sesce.\n",
      "\n",
      "DING RICHARD II:\n",
      "Teaa ma e her thet shall be sou r e th the just can deserve my \n",
      "Ahe sount th the words of aour own soul of the most stringth that hath se d \n",
      "\n",
      "DUKE OF YORK:\n",
      "Soall I am not what it make the seare that should sot fall to dissumber \n",
      "a  such aorder\n",
      "Tnd the poor servint  his nior trueset the ground of fintee will be . the with the country,\n",
      "And then the father, or the e the last coue  of the company.\n",
      "\n",
      "KING RICHARD II:\n",
      "No, no more  when theu hadt speak to him no  .\n",
      "\n",
      "GLOUCESTER:\n",
      "That thin the worl of the senet  he was not mour thoows tor mhe ring,\n",
      "The werld can ot serve a man of the crown  soill aot i the state,\n",
      "And for all a children  orth the headt tf the townrs oend  with the forter nd fand.\n",
      " ot so the reason foir of me,\n",
      "Nor late the ware ond part oo the cing  wh are tho  dod ma live,th reath with the other the work's stringe to the s mhe r striges ahn the went:\n",
      "Thr tear the e the dear lifes oe toou his not as a pass on \n",
      "Whou stand the princelof thes ausband,\n",
      "The  iht with such a curte to the strong comntryele death of nothing  the sumpee wo the hing,\n",
      "Though the devil that have no matter's daughter \n",
      "And lete tou  hanle to she aour heart   stane \n",
      "\n",
      "at not sns so \n",
      "\n",
      "Tha world th  wars of hour officer of the tente\n",
      "\n",
      "DUKE OF LORD:\n",
      "No, now, Marciuu!\n",
      "Dovwisl the fair whan he fite Ie ao thne atl the raight of a han \n",
      "\n",
      "S et the waney of the strange of the petitn of the lause \n",
      "And thou strike the consering soue the land  and lack for h the staeats:\n",
      "I have not soen the  shall not so wilh the state oith this \n",
      "Byt stand andour in eed to me heart\n",
      " he staeet not the r ling ws the caead,\n",
      "And sle a sing tf sing tm haaven  that will be so the death:\n",
      "Wheld sat th and theironsur of the thing \n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My lord, toue cau hith slanghter  I hear mie bloody labouring \n",
      "\n",
      "BENRY BOLINGBROKE:\n",
      "Te comes o make the manter  the sta hane so ns the thing:\n",
      "What shall I shall hee th the r sarring oo the leart\n",
      "\n",
      "I  te   and will do it the world,\n",
      "And he so e sattlr  the coward of the r hister \n",
      "\n",
      "Which we can sarike the sreate wan ihe poor from thar,nin the air af good father's lady io the  in his soul,\n",
      "thou wouldst not seope his marrt\n",
      "\n",
      "nd thre in a sonen wo the crown,\n",
      "Which they are seemther th the conntrnt oith our print,\n",
      "Toou wilt not see the heave  the English daspatch to shale the gods,\n",
      "That her  thou shalt sise the r strainht  f our countryent ao my wife,\n",
      "\n",
      "Den er:\n",
      "Wetl  o the oook  oo the e in the swort if the gracd.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Therefore I think thur clace with fairh to the world,\n",
      "When I am aut theu the saughter of the sing of deads,\n",
      "The laky proo torthe tongue of hiart,\n",
      "Whit thou art the firet of the dear of death:\n",
      "His soo is the waiteonnd the  tha old meavt shat shall they be cone \n",
      "\n",
      "he sord of tan th s strengt is havd, of tree the short \n",
      "And I must seale my contert tn the sense tf the common to thee  \n",
      "They have seen the stord of the fire conennr of the man thet we shall be hiate and so mies the stord \n",
      "\n",
      "DUKE OF YORK:\n",
      "Iyat we were the fost thou have to he soe rthe  mane to see the strte th his graat dustard todite\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Whyt shall I made tim to him to the death;\n",
      "Wh wher the word of tall oo see the cousin saall the stap of me cousume oe e a thousand together is my cousin,\n",
      "But  he must be so down ahe reatinest tarth,\n",
      "And with the bane oim in than the stare  kill'd the prince of the country,\n",
      "So  n heaven th this are thou find the woies\n",
      "Ao  so priam his mind, aot wand to make the  well.\n",
      "\n",
      " e embrr hand wath hea oill mand him death,\n",
      "the first be beats, Ahet thought I came to me,\n",
      "Ihe e prove my strike the land of seeet sady, Whit san this veeature in the strtes and down agd shame \n",
      "Ah  which the conscience of the love of the colour thathe r son,\n",
      "I think the mauls  and se the world, and ta the e thinks thou wert to  liven in ahe will,\n",
      "T  thou cart of tone, t d well not sto the sorrow.\n",
      "\n",
      "KING RICHARD II:\n",
      "Sof so free end tou , and to iou  words again the streeds\n",
      "\n",
      "DUKE OF YORK:\n",
      "Whe  was the field of matestais repent,\n",
      "And this sard site te l in thi world th boar the e eul of your prains.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Why, what thou hat  roan the sather stand your company,\n",
      "But I will beat a print of me hire \n",
      "And then the paye is sorvct oo korrow, and che hane oere \n",
      "  ALOWN. What should the bestemf tho stlemding ieath?\n",
      "\n",
      "BUNTOSIO:\n",
      "I shall sat so much speak the more the  to the arms,\n",
      "t of my king be sontlemen,\n",
      "That he weas  the day, and thes the  the world of the hards\n",
      "The field of the spiing ohat hight have been \n",
      "But the hotten of my provise my strte of the state,\n",
      "And tikg hith a lree the mondth\n"
     ]
    }
   ],
   "source": [
    "from hw3.answers import part1_generation_params\n",
    "\n",
    "start_seq, temperature = part1_generation_params()\n",
    "\n",
    "generated_sequence = charnn.generate_from_model(\n",
    "    model, start_seq, 10000, (char_to_idx,idx_to_char), T=0.445\n",
    ")\n",
    "\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDJZd90M2ESk"
   },
   "source": [
    "## Questions\n",
    "<a id=part1_9></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8NjmN42ESk"
   },
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:23.463915Z",
     "iopub.status.busy": "2021-12-28T07:49:23.462918Z",
     "iopub.status.idle": "2021-12-28T07:49:23.616507Z",
     "shell.execute_reply": "2021-12-28T07:49:23.616507Z"
    },
    "id": "6iAPW30J2ESk"
   },
   "outputs": [],
   "source": [
    "from cs3600.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwMROJsx2ESk"
   },
   "source": [
    "### Question 1\n",
    "Why do we split the corpus into sequences instead of training on the whole text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:23.622491Z",
     "iopub.status.busy": "2021-12-28T07:49:23.621496Z",
     "iopub.status.idle": "2021-12-28T07:49:23.763614Z",
     "shell.execute_reply": "2021-12-28T07:49:23.763614Z"
    },
    "id": "Q1D2tAer2ESk"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "There are several reasons for using sequences and not the whole corpus while training. \n",
       "    1. Memory - loading the whole text and running forward and backprop on it whole require a very large amount of memory, which is unfeasible on large corpura. \n",
       "    2. Learning the right context - with very large sequences or the entire corpus as input it will be hard for the model to learn contexts of the text. \n",
       "    3. Vanishing Gradients - Running on long sequences might cause the model to fall into the vanishing gradient problem.\n",
       "     \n",
       "Of course, splitting the corpus into very small sequences (e.g. 1) might lead to overfitting.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ4ZL2YM2ESl"
   },
   "source": [
    "### Question 2\n",
    "How is it possible that the generated text clearly shows memory longer than the sequence length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:23.768573Z",
     "iopub.status.busy": "2021-12-28T07:49:23.767574Z",
     "iopub.status.idle": "2021-12-28T07:49:23.902763Z",
     "shell.execute_reply": "2021-12-28T07:49:23.901764Z"
    },
    "id": "usRwfx8u2ESl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "** This is due to the passing the hidden layer across sequences, as the model takes in a new sequence input \n",
       "it also takes in the hidden states of the previous input, thus the model's 'memory' could showen further \n",
       "then the sequence length. **\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuuJ2Gdk2ESl"
   },
   "source": [
    "### Question 3\n",
    "Why are we not shuffling the order of batches when training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:23.907239Z",
     "iopub.status.busy": "2021-12-28T07:49:23.907239Z",
     "iopub.status.idle": "2021-12-28T07:49:24.040414Z",
     "shell.execute_reply": "2021-12-28T07:49:24.040414Z"
    },
    "id": "KgA8VGkM2ESl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "We don't shuffle the order while training because the text order has an affect on the context and so we want that order to be maintained. Otherwise we will loss the knowledge being passed by the hidden state.  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri0GWA5Z2ESl"
   },
   "source": [
    "### Question 4\n",
    "1. Why do we lower the temperature for sampling (compared to the default of $1.0$)?\n",
    "2. What happens when the temperature is very high and why?\n",
    "3. What happens when the temperature is very low and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T07:49:24.045401Z",
     "iopub.status.busy": "2021-12-28T07:49:24.044404Z",
     "iopub.status.idle": "2021-12-28T07:49:24.181038Z",
     "shell.execute_reply": "2021-12-28T07:49:24.180042Z"
    },
    "id": "sBe30h762ESl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "1. The tempature hyperparameter affects the probabilities of the chars from the softmax. During training we want the model to expriement by choosing different chars and only the most proabable ones. however, when generating text we want lower 'mistakes' and to stick to probabilities learned by the model. And so and by lowering the tempature we are giving a higher probability to models chosen chars.  \n",
       "2. When the tempature is very high there tends to be more mistakes but there is also more diversity. A very high tempature will cause the model to randomly select words from the corpus (e.g. uniform probability between words). \n",
       "3. When we lower the tempature we our increasing our models's confidence but also causing it to be more conservative in it's samples because it sticks to the more probable possiblities. When the tempature approaches 0 it is most likely to get stuck in an infinite loop, this happens because your model becomes very confident and doesn't diversify the text. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Part1_Sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (DL_class)",
   "language": "python",
   "name": "pycharm-b9ffc24f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}